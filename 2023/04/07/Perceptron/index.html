<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="PrefaceContinuing from the last article in our “Tracing the Roots” series, we’ll delve into a pioneering work in the field of artificial intelligence that followed the MP neuron model: the Perceptron.">
<meta property="og:type" content="article">
<meta property="og:title" content="Tracing Back to the Roots | &quot;Perceptron&quot; The Intelligent Machine (1957)">
<meta property="og:url" content="https://zixin2006.github.io/2023/04/07/Perceptron/index.html">
<meta property="og:site_name" content="Zixin&#39;s Multiverse.">
<meta property="og:description" content="PrefaceContinuing from the last article in our “Tracing the Roots” series, we’ll delve into a pioneering work in the field of artificial intelligence that followed the MP neuron model: the Perceptron.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zixin2006.github.io/images/tbto_21.png">
<meta property="og:image" content="https://zixin2006.github.io/images/tbto_22.png">
<meta property="og:image" content="https://zixin2006.github.io/images/tbto_23.png">
<meta property="og:image" content="https://zixin2006.github.io/images/tbto_24.png">
<meta property="og:image" content="https://zixin2006.github.io/images/tbto_25.png">
<meta property="og:image" content="https://zixin2006.github.io/images/tbto_26.png">
<meta property="og:image" content="https://zixin2006.github.io/images/tbto_27.png">
<meta property="article:published_time" content="2023-04-07T03:30:42.000Z">
<meta property="article:modified_time" content="2024-11-11T01:25:34.879Z">
<meta property="article:author" content="Zixin Yu">
<meta property="article:tag" content="Academics">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zixin2006.github.io/images/tbto_21.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/me.png">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/lib/meslo-LG/styles.css">

    
<link rel="stylesheet" href="/lib/justified-gallery/justifiedGallery.min.css">

    
<link rel="stylesheet" href="/css/style.css">

    <!-- rss -->
    
    
    <!-- jquery -->
    
<script src="/lib/jquery/jquery.min.js"></script>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.1.1"></head>

<body>
    <div class="banner">
<div id="blogtitel" class="blogtitel">Zixin&#39;s Multiverse.</div>
<ul id="wrapper" class="wrapper">
  <div class="sun">
    <div class="star"></div>
  </div>
  <div class="mercury">
    <div class="planet">
      <div class="shadow"></div>
    </div>
  </div>
  <div class="venus">
    <div class="planet">
      <div class="shadow"></div>
    </div>
  </div>
  <div class="earth">
    <div class="planet"><div class="shadow"></div></div>
  </div>
  <div class="mars">
    <div class="planet"><div class="shadow"></div></div>
  </div>
  <div class="jupiter">
    <div class="planet"><div class="shadow"></div></div>
  </div>
</ul>
</div>

    <div class="background">
      
        <div id="header-post">
  <a id="menu-icon" href="#"><i class="fa fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fa fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a target="_blank" rel="noopener" href="http://github.com/zixin2006">Projects</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2023/05/01/Backprop/"><i class="fa fa-chevron-left" aria-hidden="true" onmouseover='$("#i-prev").toggle();' onmouseout='$("#i-prev").toggle();'></i></a></li>
        
        
        <li><a class="icon" href="/2023/03/29/MP_neuron/"><i class="fa fa-chevron-right" aria-hidden="true" onmouseover='$("#i-next").toggle();' onmouseout='$("#i-next").toggle();'></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa fa-chevron-up" aria-hidden="true" onmouseover='$("#i-top").toggle();' onmouseout='$("#i-top").toggle();'></i></a></li>
        <li><a class="icon" href="#"><i class="fa fa-share-alt" aria-hidden="true" onmouseover='$("#i-share").toggle();' onmouseout='$("#i-share").toggle();' onclick='$("#share").toggle();return false;'></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://zixin2006.github.io/2023/04/07/Perceptron/"><i class="fa fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://zixin2006.github.io/2023/04/07/Perceptron/&text=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)"><i class="fa fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://zixin2006.github.io/2023/04/07/Perceptron/&title=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)"><i class="fa fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://zixin2006.github.io/2023/04/07/Perceptron/&is_video=false&description=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)"><i class="fa fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)&body=Check out this article: https://zixin2006.github.io/2023/04/07/Perceptron/"><i class="fa fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://zixin2006.github.io/2023/04/07/Perceptron/&title=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)"><i class="fa fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://zixin2006.github.io/2023/04/07/Perceptron/&title=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)"><i class="fa fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://zixin2006.github.io/2023/04/07/Perceptron/&title=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)"><i class="fa fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://zixin2006.github.io/2023/04/07/Perceptron/&title=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)"><i class="fa fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://zixin2006.github.io/2023/04/07/Perceptron/&name=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)&description="><i class="fa fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      
    </div>
  </span>
</div>

      
      <div class="content index width mx-auto px2 my4">
          
          <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Zixin's Multiverse.</span>
      </span>
      
    <div class="postdate">
        <time datetime="2023-04-07T03:30:42.000Z" itemprop="datePublished">2023-04-07</time>
    </div>


      
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link-link" href="/tags/Academics/" rel="tag">Academics</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p><strong>Preface</strong><br>Continuing from the last article in our “Tracing the Roots” series, we’ll delve into a pioneering work in the field of artificial intelligence that followed the MP neuron model: the <strong>Perceptron</strong>. This article will give a detailed analysis of the 1957 paper, <em>The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain</em>, by Frank Rosenblatt, a psychologist and neuroscientist at Cornell University.</p>
<p><strong>Introduction</strong><br>At the beginning of the article, the author presents three fundamental questions for understanding cognition, generalization, memory, and thought:</p>
<ol>
<li>How do biological systems perceive or detect information from the physical world?</li>
<li>In what form is information stored in memory?</li>
<li>How does stored information influence recognition and behavior?</li>
</ol>
<p>The first question has largely been addressed in sensory physiology. For the second and third questions, the author discusses two perspectives:</p>
<ul>
<li><strong>The Coded Memory Hypothesis</strong> suggests that information is stored in an encoded form, like a wiring diagram, that can directly translate sensory input into memory. Recognizing external stimuli involves matching the current sensory pattern with stored content and mapping it to a corresponding response.</li>
<li><strong>The Empiricist Memory Hypothesis</strong> proposes that information storage does not involve specific encoding but occurs through forming new connections in the nervous system. Since the information is stored as neural connections, new stimuli use these established pathways, triggering an appropriate response without needing a separate recognition process.</li>
</ul>
<p><img src="/images/tbto_21.png" alt="Image"></p>
<p>These two hypotheses can actually be mapped to two major schools of artificial intelligence: <strong>Connectionism</strong> and <strong>Symbolism</strong>. Symbolism assumes cognitive processes are achieved through clear symbol manipulation and sequential application of rules. In contrast, Connectionism models cognition by simulating neural networks, relying on distributed representations and statistical methods.</p>
<p>In the previous article, we discussed MP neurons, which are based on Boolean logic and binary operations. In the <em>Perceptron</em> paper, the author critiques symbolic methods. These theorists focus on implementing functions like perception and memory using deterministic physical systems rather than studying how the brain actually performs them. Many of the proposed models fail in important ways: they lack consistency across different situations (equivalence), they don’t use neural resources efficiently (neuroeconomy), they impose strict requirements on connections (excessive specificity), and the variables in the models lack biological evidence. Supporters of these physical systems argue that biological intelligence can be replicated by improving existing principles. <strong>However, the author believes these limitations show that models not based on biological systems can never explain biological intelligence, as the difference in principles is clear.</strong></p>
<p>Conversely, studies that focus on biological systems often lack precision and rigor, making it difficult to assess whether the described systems can actually work in real neural networks. <strong>The lack of an analytical language as effective as Boolean algebra is another obstacle.</strong></p>
<p>To address these issues, the author first introduces several key assumptions:</p>
<ol>
<li>The construction of the neural system’s initial network is largely random, with minimal genetic restrictions.</li>
<li>Neural connections exhibit some plasticity. After a period of neural activity, the probability of stimulating one group of cells and triggering a response in another group changes due to long-term changes in neurons.</li>
<li>Similar stimuli tend to form pathways to the same responsive cells, and vice versa.</li>
<li>Positive and negative reinforcement promotes or inhibits the formation of connections.</li>
<li>Similarity isn’t an inherent attribute of specific stimuli but depends on the physical organization of the perceptual system.</li>
</ol>
<p>These assumptions will serve as essential foundations for the model. Unlike previous work, <strong>the Perceptron uses a probabilistic model rather than Boolean operations.</strong></p>
<p><strong>The Basic Structure of the Perceptron</strong><br>The following is a simple diagram of the perceptron’s structure:</p>
<p><img src="/images/tbto_22.png" alt="Image"></p>
<center>Fig. 2: A Simple Diagram of the Perceptron Structure.</center>

<p>The model is organized into four parts:</p>
<ol>
<li><strong>Sensory Input Layer</strong>: The perceptron’s input comes from sensory receptors, like retinal points, labeled S-points, that respond in an “all-or-none” manner to stimuli. Note: “All-or-none” means that if the stimulus weight exceeds the neuron’s threshold $\theta$, the neuron activates; if not, it remains inactive.</li>
<li><strong>Projection Area</strong>: The S-points send signals to a projection area (a group of associated cells labeled $A_1$, with neurons referred to as A units). The activation of A units follows the same “all-or-none” rule. Notice the <strong>localized connections</strong> here: A units’ source points tend to cluster around each A unit’s central retinal point. The number of origin points decreases exponentially with distance from the central point. This distribution is vital in <strong>contour detection</strong>, making it a <strong>bio-inspired design</strong>. Sometimes, the projection area is omitted during modeling.</li>
<li><strong>Association Area</strong>: Each A unit in the association area receives signals from multiple source points, and connections between the two regions are <strong>random</strong>.</li>
<li><strong>Response Layer</strong>: Outputs from the association area go to response cells (labeled R units). While the perceptron uses feedforward connections until the association area, feedback is provided in the response layer. The author suggests two almost equivalent feedback mechanisms:<ul>
<li>(a) Excitatory feedback to the source cell set of an R unit.</li>
<li>(b) Inhibitory feedback to the complement of the source set of an R unit.</li>
</ul>
</li>
</ol>
<p>In this system, based on feedback, the neurons’ responses are <strong>mutually exclusive</strong>. If one response unit $R_1$ is activated, it inhibits the set of cells connected to another response unit $R_2$, preventing $R_2$ from responding. As reinforcement and inhibition continue, the system gradually adapts to stimuli of different categories—a process we describe as <strong>learning ability</strong>.</p>
<p>For simplification in later analysis, only <strong>Sensory Input - Association Area - Response Layer</strong> are kept. The A unit represents neurons in the association area, and the R unit represents neurons in the response layer. To simplify further, the author distinguishes between two response phases:</p>
<p><img src="/images/tbto_23.png" alt="Image"></p>
<center>Fig. 3: Response Phases to Stimuli.</center>

<ul>
<li><strong>Predominant Phase</strong>: A units in the system respond to stimuli, but R units remain inactive. This phase is temporary and transitions to the Postdominant phase.</li>
<li><strong>Postdominant Phase</strong>: One R unit becomes active, dominating by suppressing other activities in its source set.</li>
</ul>
<p>In the predominant phase, responses are random. However, as stimulus-response connections are reinforced, the system learns to respond to specific stimuli. Below, we discuss the characteristics of each neuron in the macrostructure.</p>
<p><strong>Modeling Neuron Characteristics</strong></p>
<p>To address assumption two (neural plasticity) and to enable the Perceptron’s dynamic learning process, we introduce a model for neuron characteristics, which will serve as the foundation for subsequent simulations.</p>
<p>Assume that each A unit’s output impulse can be represented by a value $V$, which could reflect amplitude, frequency, delay, or transmission probability. A higher value indicates that all output impulses of an A unit are more effective or more likely to reach the response layer. Each A unit’s value is assumed to be relatively stable (determined by the cell membrane and metabolic state of the cell), but it is not constant. We generally assume that <strong>activity increases the cell’s value, while inactivity decreases it</strong>.</p>
<p>An interesting model is one where cells compete for metabolic materials, with more active cells drawing from inactive ones. In such a system, without activity, all cells’ states would stabilize, resulting in a net value balance across the system. The author introduces three systems ($\alpha, \beta, \gamma$) with different rules for changes in neuron values $V$:</p>
<p><img src="/images/tbto_24.png" alt="Image"></p>
<center>Fig. 4: Comparison of Three System Logic Characteristics.</center>

<ul>
<li><strong>$\alpha$ System (Uncompensated Gain System)</strong>: Each time an A unit receives a stimulus, it gets a fixed gain, accumulating over time without decreasing. This cumulative gain mechanism is suited for tasks with long-term accumulation effects.</li>
<li><strong>$\beta$ System (Constant Feed System)</strong>: Each source set gains at a constant rate, with distribution proportional to the activity in the set. Non-dominant set cells also receive gains, ensuring all units have the chance to activate and strengthen over time. This balanced mechanism supports ongoing learning and stable gain distribution.</li>
<li><strong>$\gamma$ System (Parasitic Gain System)</strong>: Active cells gain $V$ values at the expense of inactive ones, keeping the total source set value constant. This competitive mechanism is suitable for tasks focused on resource optimization.</li>
</ul>
<p>The next section builds learning curves and performs a sensitivity analysis on the model.</p>
<p><strong>Model Analysis of the Predominant Phase</strong></p>
<p>To compare learning curves, we define two key metrics:</p>
<ul>
<li><p><strong>$P_a$</strong>: The expected proportion of A units activated by a given stimulus. This is calculated by summing over all possible excitatory ($e$) and inhibitory ($i$) inputs under the condition that the <strong>sum of excitation minus inhibition reaches or exceeds</strong> the threshold $\theta$ ($e - i \geq \theta$). $P(e, i)$ represents the joint probability of excitatory and inhibitory components; $x$ is the total excitatory connections per A unit, $y$ is the total inhibitory connections, and $R$ is the proportion of S-points activated before each A unit.<br>$$<br>\begin{align*}<br>P_a &amp;&#x3D; \sum_{e&#x3D;\theta}^x \sum_{i&#x3D;0}^{\min(y, e-\theta)} P(e, i) \<br>P(e, i) &amp;&#x3D; \binom{x}{e} R^e (1 - R)^{x - e} \times \binom{y}{i} R^i (1 - R)^{y - i}<br>\end{align*}<br>$$</p>
</li>
<li><p><strong>$P_c$</strong>: The conditional probability that an A unit responding to a stimulus $S_1$ will also respond to a different stimulus $S_2$:<br>$$<br>P_c &#x3D; \frac{1}{P_a} \sum_{e&#x3D;\theta}^x \sum_{i&#x3D;e-\theta}^y \sum_{l_e&#x3D;0}^e \sum_{l_i&#x3D;0}^i \sum_{g_e&#x3D;0}^{x-e} \sum_{g_i&#x3D;0}^{y-i} P(e, i, l_e, l_i, g_e, g_i)<br>$$<br>Here, $l_e$ and $l_i$ are the counts of excitatory and inhibitory source points “lost” when $S_1$ is replaced by $S_2$, while $g_e$ and $g_i$ are the counts of points “gained” when $S_1$ is replaced by $S_2$. The joint probability $P(e, i, \ell_e, \ell_i, g_e, g_i)$ is as follows:<br>$$<br>P(e, i, \ell_e, \ell_i, g_e, g_i) &#x3D; \binom{x}{e} R^e (1 - R)^{x - e} \times \binom{y}{i} R^i (1 - R)^{y - i} \times \binom{e}{\ell_e} L^{\ell_e} (1 - L)^{e - \ell_e} \times \binom{i}{\ell_i} L^{\ell_i} (1 - L)^{i - \ell_i} \times \binom{x - e}{g_e} G^{g_e} (1 - G)^{x - e - g_e} \times \binom{y - i}{g_i} G^{g_i} (1 - G)^{y - i - g_i}<br>$$<br>Here, $L$ is the proportion of S-points lit by $S_1$ but not $S_2$, and $G$ is the proportion of S-points remaining from $S_1$ that are included in $S_2$. Let’s examine how $P_a$ and $P_c$ change with parameter variations.</p>
</li>
</ul>
<p><strong>Sensitivity Analysis of $P_a$</strong></p>
<p>Using the formula and rules discussed, we can plot graphs (code available in repository):</p>
<p><img src="/images/tbto_25.png" alt="Image"></p>
<center>Fig. 5: Variation of $P_a$ with the proportion of retina area illuminated (R).</center>

<p>Conclusions:</p>
<ol>
<li>Increasing the threshold $\theta$ or increasing inhibitory connections $y$ reduces $P_a$.</li>
<li>When excitatory and inhibitory inputs are roughly balanced, the $P_a$ curve flattens with changes in $R$.</li>
<li>Systems with similar amounts of excitatory and inhibitory input converge faster. For optimal stability, a balance of excitatory and inhibitory input is desirable.</li>
</ol>
<p><strong>Sensitivity Analysis of $P_c$</strong></p>
<p><img src="/images/tbto_26.png" alt="Image"></p>
<center>Fig. 6: Variation of $P_c$ with $R$.</center>

<ol>
<li>As $\theta$ increases, $P_c$ decreases faster than $P_a$.</li>
<li>Like $P_a$, $P_c$ decreases as the proportion of inhibitory connections $y$ increases.</li>
</ol>
<p>Additionally, the author explored how $P_c$ changes with stimulus overlap $C$, as shown below:</p>
<p><img src="/images/tbto_27.png" alt="Image"></p>
<center>Fig. 7: Variation of $P_c$ with $C$. Solid line represents $R=0.5$, dashed line represents $R=0.2$, with $x=10$ and $y=0$.</center>

<ol>
<li>Even when stimuli are entirely non-overlapping, $P_c$ remains above zero, showing that the system can still respond to completely different stimuli.</li>
<li>As overlap increases, $P_c$ approaches 1, indicating strong consistency in system response.</li>
<li>Higher threshold values result in lower $P_c$ values compared to lower thresholds.</li>
</ol>
<p>Analyzing $P_a$ and $P_c$ provides a theoretical basis for quantitative analysis and parameter tuning. Summing up the insights gained:</p>
<ul>
<li>Symbolism vs. Connectionism</li>
<li>Transition from formal logic to probabilistic models in neural networks</li>
<li>The foundational structure and functioning of the perceptron</li>
<li>Neuron characteristic modeling with three systems</li>
<li>Learning curve development and sensitivity analysis</li>
<li>Convergence based on different criteria</li>
</ul>
<p>The second half of the original paper focuses on the perceptron’s spontaneous organization, memory and learning capabilities, and the model’s performance in varied environments. We won’t cover this section here, as it primarily involves <strong>small-scale experiments and fine-tuning</strong> rather than the foundational contributions of the first half.</p>
<p>A major limitation of the Perceptron is that it can only solve linearly separable tasks, struggling with non-linear ones. This led to the development of the <strong>Multilayer Perceptron</strong> with non-linear activation functions in hidden layers, allowing for non-linear classification. However, challenges remain, especially with <strong>training</strong>. Next time, we’ll explore the 1986 paper by Hinton, Rumelhart, and Williams, <em>Learning Representations by Back-Propagating Errors</em>, which made training deep neural networks feasible.</p>
<p>(<em>P.S. Thanks for reading! A like would mean the world 😊</em>)</p>

  </div>
</article>



      </div>
      
       <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a target="_blank" rel="noopener" href="http://github.com/zixin2006">Projects</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://zixin2006.github.io/2023/04/07/Perceptron/"><i class="fa fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://zixin2006.github.io/2023/04/07/Perceptron/&text=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)"><i class="fa fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://zixin2006.github.io/2023/04/07/Perceptron/&title=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)"><i class="fa fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://zixin2006.github.io/2023/04/07/Perceptron/&is_video=false&description=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)"><i class="fa fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)&body=Check out this article: https://zixin2006.github.io/2023/04/07/Perceptron/"><i class="fa fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://zixin2006.github.io/2023/04/07/Perceptron/&title=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)"><i class="fa fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://zixin2006.github.io/2023/04/07/Perceptron/&title=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)"><i class="fa fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://zixin2006.github.io/2023/04/07/Perceptron/&title=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)"><i class="fa fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://zixin2006.github.io/2023/04/07/Perceptron/&title=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)"><i class="fa fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://zixin2006.github.io/2023/04/07/Perceptron/&name=Tracing Back to the Roots | &#34;Perceptron&#34; The Intelligent Machine (1957)&description="><i class="fa fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
      <ul>
        <li id="toc"><a class="icon" href="#" onclick='$("#toc-footer").toggle();return false;'><i class="fa fa-list fa-lg" aria-hidden="true"></i> TOC</a></li>
        <li id="share"><a class="icon" href="#" onclick='$("#share-footer").toggle();return false;'><i class="fa fa-share-alt fa-lg" aria-hidden="true"></i> Share</a></li>
        <li id="top" style="display:none"><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a></li>
        <li id="menu"><a class="icon" href="#" onclick='$("#nav-footer").toggle();return false;'><i class="fa fa-bars fa-lg" aria-hidden="true"></i> Menu</a></li>
      </ul>
    </div>

  </div>
</div>

      
      <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2024 Zixin Yu
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a target="_blank" rel="noopener" href="http://github.com/zixin2006">Projects</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

      
<script src="/lib/justified-gallery/jquery.justifiedGallery.min.js"></script>


<script src="/js/main.js"></script>

<!-- Google Analytics -->

<!-- Disqus Comments -->


    </div>
</body>
</html>
